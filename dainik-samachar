1. Today I worked on cost optimisation where I started analyzing the billing patterns for resources in GCP for last 90 days
-Based on my findings there are certain resources where I need to investigate the behaviour of applications running in UAT environment.

2. Today I went to confluence and started creating notes on our existing infrastructure.
-Based on those notes I can suggest some strategies to improve our infrastructure.

---------------------------------------------------------------------------------

Meeting Notes:

1. So far I am working bucket object deletion. One of the bucket objects has been deleted and second bucket process has been initialed.
2. On cost optimization task. Last week I have analyzed the compute engines workloads for anomalies and any changes.
Same I did for composer based clusters where multiple nodes are bearing the workloads. Now to get more in depth knowledge on this. 
I have tried scheduling a meeting with Liang but he's on vacation. So now for this week I am going to have meeting with Satish and Linlin on the same matter.
Once I will get a better understandin of workloads in the UAT environment then I can create a report on cost optimization strategies what we can implement.
3. Apart from this Last week I had few meeting with Yihan to resolve the session timeout error. 
So far the load test looks fine the session timeout error occurance has been significantly reduced. But still satish and team are still performing some load tests to be sure.


-------------------------------------------------------------------------------------------

Composer2:

1. As per the meeting with linlin and sathish, there are certain load testing going on UAT enviroment for composer2.
2. They have started these test from mid september and they are expecting it to continue testing until end of October.
3. Currently they're running 150 to 500 sessions each day in load testing which is expected to continue in the prod environment as well.
4. Once testing is done they are planning to implement composer2 in production environment.
5. My conclusion by understanding the composer2 billing patterns and workload is that, we can optimize it on a level of machine configuration which will happen during the time of resource provisioning in production environment.
6. I am planning to collaborate with Liang on this and try to optimize the composer2 so that it can run efficiently and we can save some cost.


Cloud Logger:

1. As per the meeting with Linlin and other team members yesterday, I undeerstand that there are alot of error logs accumulated in GCP.
2. These extra logs are not useful and we need to find out a way to filter them.
3. Once we can identify and filter them then we can actually migrate the important logs to Datadog.
4. Another way is we can ignore the migration of logs to save cost and create a new dashboard specifically for Log monitoring in GCP itself.
5. In order to accomplish this task I need to collaborate with Sathish and team to understand the use of these logs better.
6. And then I can find a way to filter them and use the important logs to create a dashboard in GCP.

Compute Engine:

1. So far I have Identified few VMs which are not in use and are in Idle state. So we can remove them.
2. Along with that few VMs are not working on high load task. So for those we can down grade there configurations.
3. Mostly VMs what I found are basically nodes for GKE cluster. So I need to investigate there configuration and behaviour furthermore.


-----------------------------------------------------------------------------------------------

2024-10-18

1.Today I have worked on composer2 workload setup and deployment with Liang and team.
  -I have updated multiple secret entries for DAG processes to run.
  -I have updated connection configuration for certain SQL servers for concurrent connection.
  -I have updated performance configurations (CPU, Memory) for certain SQL servers for better worload handling.

2.Today I also worked on cloud log analysis to find out potential logs which we can omette from Cloud logger.

-----------------------------------------------------------------------------------------------

2024-10-21

1. Today I have colaborated with Sathish and Conor to resolve the error log issue.
  - We have identified the python package which was resposible for the unnecessary error log generation.
  - Conor was the package manager so he update the logging configurations of the package.
  - This will directly affect the daily error log generation process in Cloud logger.
  - Now we are planning to test it in UAT and DEV environment soon.

2. Today I have completed the first phase of "GCP Cost Optimisation Report"
  - I have included all 4 major services which were resposible for moajority of cost consumption since last few months.
  - Those are GCP Cloud Composer2, Compute, Storage and Logger.
  - I have included 4 sections for each service in the report.
  - Section:
    - Current Status
    - Action Plan
    - Progress
    - Recommendations

-----------------------------------------------------------------------------------------------------


2024-10-22

1. Initiated working on resource labeling in GCP for all d3-ihub (dev/uat/prod) as well as intuative-hipaavault (dev/uat/prod) environments.
  - First we are going to identify the respective terraform code for each resource and then start labeling process via terraform.
2. Initiated the research for Zoom connector setup in our GCP on premises environment.
  - Creating a list of resources and there components which are going to utilize in this process.
3. Worked on ticket CAIN-8984 requested by Sathish.
  - Updated the terraform code to add SSH level access for accounts in Prod environment.
4. Worked on ticket CAIN-8985 requested by Sathish.
  - Upgraded the composer2 configuration for CPU utilization (Worker and Scheduler) through GCP console.
  - Tested the configuration by running multiple DAGS in composer2 and monitored it.


------------------------------------------------------------------------------------------------------

2024-10-23

1. Identified the core terraform code for resource labeling ticket CAIN-8951.
  - Had a discussion with Liang on execution plan.
  - Create the list of resources targeted for labeling.
  - Initiating the labeling in d3-ihub environment from 23rd Oct.
2. Initiated work on ticket CAIN-8995 requested Sathish.
3. Initated working on ticket CAIN-8992 Case Insight GKE monitoring - accessing control.
  - Standardizing terraform CI/CD for dataflow job.
  - Using seperate service account for dataflow and datadog


------------------------------------------------------------------------------------------------------


2024-10-24

- Worked on ticket CAIN-8951.
  - Started labeling in d3-ihub and d3-shared environments.
  - Completed resources in d3-ihub:
    - Compute: ihub-dev-vm
    - Database: ihub-database
    - GKE: ihub-gke-cluster
  - Completed resources in d3-shared:
    - Compute: d3-shared-vm
  - Continuing labeling process for other resources.
- Completed ticket CAIN-8995 to add new branches (develop and master) in multiple repositories.

-----------------------------------------------------------------------------------------------------------


https://bgiri-gcloud.medium.com/how-to-set-up-terraform-on-your-local-laptop-to-connect-with-google-cloud-625ec98c15c4
